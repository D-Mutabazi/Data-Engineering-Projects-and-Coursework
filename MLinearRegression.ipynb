{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "\n",
    "\n",
    "\n",
    "import talib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 338\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid search completed and results saved to CSV!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    336\u001b[0m \u001b[38;5;66;03m############################### HYPER-PARAMETER TUNING ###############################\u001b[39;00m\n\u001b[0;32m--> 338\u001b[0m \u001b[43mgrid_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#start search\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 247\u001b[0m, in \u001b[0;36mgrid_search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrid_search\u001b[39m():\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;66;03m# Initialize CSV to store results\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhyperparameter_tuning_results.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m     \u001b[43minitialize_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Define hyperparameter grid\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     features \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOpen\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHigh\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLow\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50_sma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m200_sma\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m50_ema\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    251\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m100_ema\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMACD_line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSignal_line\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mADX\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRSI\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch_k\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    252\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstoch_d\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mATR\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[1], line 166\u001b[0m, in \u001b[0;36minitialize_csv\u001b[0;34m(file_name)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minitialize_csv\u001b[39m(file_name):\n\u001b[1;32m    162\u001b[0m     headers \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlookback_window\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeatures_used\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m    163\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAPE_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMBE_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_1_day\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    164\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAPE_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMBE_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_3_day\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    165\u001b[0m                \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSE_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAE_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMAPE_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMBE_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRMSE_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mR2_5_day\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m--> 166\u001b[0m     df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241m.\u001b[39mDataFrame(columns\u001b[38;5;241m=\u001b[39mheaders)\n\u001b[1;32m    167\u001b[0m     df\u001b[38;5;241m.\u001b[39mto_csv(file_name, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the Linear Regression model- to support multi-output\n",
    "class LinearRegressionMultiOutputBatch:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=1, batch_size=32):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs  # Number of passes over the entire dataset\n",
    "        self.batch_size = batch_size  # Size of each mini-batch\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        num_outputs = y.shape[1] if y.ndim > 1 else 1\n",
    "\n",
    "        # Initialize weights and biases for multiple outputs\n",
    "        self.weights = np.zeros((num_features, num_outputs))  # Shape: (num_features, num_outputs)\n",
    "        self.bias = np.zeros(num_outputs)  # Shape: (num_outputs,)\n",
    "\n",
    "        num_batches = int(np.ceil(num_samples / self.batch_size))\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "        \n",
    "            # Process the data in batches without shuffling\n",
    "            for batch_idx in range(num_batches):\n",
    "                # Determine batch start and end indices\n",
    "                start_idx = batch_idx * self.batch_size\n",
    "                end_idx = min(start_idx + self.batch_size, num_samples)\n",
    "\n",
    "                # Extract the batch data\n",
    "                X_batch = X[start_idx:end_idx]\n",
    "                y_batch = y[start_idx:end_idx]\n",
    "\n",
    "                # Calculate predictions: Shape of y_pred -> (batch_size, num_outputs)\n",
    "                y_pred = np.dot(X_batch, self.weights) + self.bias\n",
    "\n",
    "                # Compute gradients\n",
    "                batch_size_actual = X_batch.shape[0]  # Adjust for last batch\n",
    "\n",
    "                residuals = y_pred - y_batch  # Shape: (batch_size, num_outputs)\n",
    "\n",
    "                dw = (1 / batch_size_actual) * np.dot(X_batch.T, residuals)  # Shape: (num_features, num_outputs)\n",
    "                db = (1 / batch_size_actual) * np.sum(residuals, axis=0)  # Shape: (num_outputs,)\n",
    "\n",
    "                # Update weights and biases\n",
    "                self.weights -= self.learning_rate * dw\n",
    "                self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias  # Shape: (num_samples, num_outputs)\n",
    "\n",
    "# Feature Engineering \n",
    "def multivariateFeatureEngineering(data):\n",
    "    \n",
    "    #Trend following Indicators:\n",
    "\n",
    "    #SMA - identofy long term trend\n",
    "    data['50_sma'] = data['Close'].rolling(window=50).mean() \n",
    "    data['200_sma'] = data['Close'].rolling(window=200).mean() \n",
    "\n",
    "    #EMA - trend analysis: more weight applied to recent points\n",
    "    data['50_ema'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "    data['100_ema'] = data['Close'].ewm(span=100, adjust=False).mean()\n",
    "\n",
    "    #MACD\n",
    "    data['12_ema'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['26_ema'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    data['MACD_line'] = data['12_ema']-data['26_ema'] # calculate the MACD line\n",
    "    data['Signal_line'] = data['MACD_line'].ewm(span=9, adjust=False).mean() # 9-preiod ema signal calculated from the Macdline\n",
    "    # data['MACD_histogram'] = data['MACD_line'] - data['Signal_line']\n",
    "\n",
    "    #ADX\n",
    "    # Calculate ADX using TA-Lib (14-period by default)\n",
    "    data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Momentum indicators:\n",
    "\n",
    "    #RSI - 14-period\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    \n",
    "    #Stochastic Oscillator\n",
    "    data['stoch_k'], data['stoch_d'] = talib.STOCH(data['High'], data['Low'], data['Close'], \n",
    "                                                fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    #Volatility indicators#:\n",
    "\n",
    "    #ATR -Default period for ATR is 14\n",
    "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    # print(dataset.isnull().any(axis=1).sum())\n",
    "    print(data.head(50))\n",
    "    data = data.dropna() # drop rows that have NA\n",
    "\n",
    "    #drop certain featires\n",
    "    data = data.drop(columns=['12_ema', '26_ema'])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def multivariateFeatureLagMultiStep(data, n_past, future_steps, target_column=1):\n",
    "    features = []\n",
    "    response = []\n",
    "\n",
    "    max_future_step = max(future_steps)\n",
    "    num_features = data.shape[1]\n",
    "    group_feature_lags =  1 # change grouping of lagged features\n",
    "\n",
    "    # Adjust the loop to prevent index out of bounds\n",
    "    for i in range(n_past, len(data) - max_future_step + 1):\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "                \n",
    "            lagged_features = []\n",
    "\n",
    "            for feature_idx in range(num_features):\n",
    "                feature_lags = data.iloc[i - n_past:i, feature_idx].values \n",
    "                lagged_features.extend(feature_lags) \n",
    "\n",
    "        elif group_feature_lags==0:\n",
    "            features.append(data.iloc[i - n_past:i, :].values)  # Take all columns as features\n",
    "\n",
    "        # Use .iloc for integer-based indexing and .values to get a NumPy array\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "            features.append(lagged_features)\n",
    "\n",
    "        # Extract the target values at specified future steps using .iloc\n",
    "        response.append([data.iloc[i + step - 1, target_column] for step in future_steps])\n",
    "\n",
    "    # Convert lists to NumPy arrays after the loop\n",
    "    features = np.array(features)  # Shape: (num_samples, n_past, num_features)\n",
    "    response = np.array(response)  # Shape: (num_samples, len(future_steps))\n",
    "\n",
    "    # Flatten the features to 2D array: (num_samples, n_past * num_features)\n",
    "    features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    return features_flat, response\n",
    "\n",
    "\n",
    "# Data loading\n",
    "def data_loader(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index('Time', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to evaluate model predictions\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mbe = np.mean(y_pred - y_true)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "\n",
    "# Initialize CSV file and write headers\n",
    "# Initialize CSV file and write headers\n",
    "def initialize_csv(file_name):\n",
    "    headers = ['lookback_window', 'features_used', 'learning_rate', 'batch_size', 'epochs', \n",
    "               'MSE_1_day', 'MAE_1_day', 'MAPE_1_day', 'MBE_1_day', 'RMSE_1_day', 'R2_1_day',\n",
    "               'MSE_3_day', 'MAE_3_day', 'MAPE_3_day', 'MBE_3_day', 'RMSE_3_day', 'R2_3_day',\n",
    "               'MSE_5_day', 'MAE_5_day', 'MAPE_5_day', 'MBE_5_day', 'RMSE_5_day', 'R2_5_day']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Append a single result (row) to the CSV file\n",
    "def append_result_to_csv(file_name, result):\n",
    "    df = pd.DataFrame([result])  # Convert result dictionary to DataFrame\n",
    "    df.to_csv(file_name, mode='a', header=False, index=False)  # Append to CS\n",
    "\n",
    "#                                    /* Feature combinations*/\n",
    "def featuresComblist(features):\n",
    "    import itertools\n",
    "\n",
    "    initial_feature = ['Close'] # Starting with the closing price\n",
    "\n",
    "    # Get all combinations of the features list and add to the initial feature (Closing Price)\n",
    "    feature_combinations = []\n",
    "    for i in range(len(features) + 1):\n",
    "        for combination in itertools.combinations(features, i):\n",
    "            feature_combinations.append(list(combination)+ initial_feature )\n",
    "    \n",
    "    return feature_combinations\n",
    "\n",
    "############################################# Load data and Feature Engineer ###############################################\n",
    "\n",
    "# filepath = './Data/EURUSD_D1.csv' \n",
    "# dataset = data_loader(filepath)\n",
    "\n",
    "# # Generate additional Features \n",
    "# multiVarData = multivariateFeatureEngineering(dataset) \n",
    "# cols  = [col for col in multiVarData.columns if col!='Close'] + ['Close'] # Put target on End\n",
    "# multiVarData = multiVarData[cols]\n",
    "\n",
    "\n",
    "# ################################  Generate different hyper-parameter values for grid search #################################\n",
    "\n",
    "\n",
    "# features = ['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "#        '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "#        'stoch_d', 'ATR']\n",
    "\n",
    "# feature_combinations = featuresComblist(features) # feature cominations\n",
    "# lookback_windows =  [1 , 3, 5, 7, 10]  # look back window\n",
    "# future_steps = [1, 3, 5]  # model outputs\n",
    "# learning_rate = [0.1, 0.01, 0.001] \n",
    "# batch_sizes = [16, 32, 64]\n",
    "# epochs = [20, 50, 100]\n",
    "\n",
    "# target_col = -1  # response\n",
    "\n",
    "# ######## build model based on this combinato\n",
    "\n",
    "# features, response = multivariateFeatureLagMultiStep(multiVarData, n_past,future_steps,  target_col)\n",
    "\n",
    "# #################################### Build Forecast model ########################\n",
    "\n",
    "# #train test split\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.2, random_state=12, shuffle=False)\n",
    "\n",
    "# scaler = StandardScaler()\n",
    "# X_train = scaler.fit_transform(X_train)\n",
    "# X_test = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# n_epochs = 5\n",
    "# batch_size =  32\n",
    "# model_lr = LinearRegressionMultiOutputBatch(learning_rate, n_epochs,batch_size)\n",
    "# model_lr.fit(X_train, Y_train)\n",
    "\n",
    "# predict = model_lr.predict(X_test)\n",
    "# print(predict.shape, Y_test.shape)\n",
    "\n",
    "\n",
    "\n",
    "############################ Hyper-Paramert Tuning: Grid Search #######################\n",
    "\n",
    "\n",
    "# Grid search through all combinations of hyperparameters\n",
    "def grid_search():\n",
    "    # Initialize CSV to store results\n",
    "    file_name = 'hyperparameter_tuning_results.csv'\n",
    "    initialize_csv(file_name)\n",
    "    \n",
    "    # Define hyperparameter grid\n",
    "    features = ['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "                '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "                'stoch_d', 'ATR']\n",
    "    \n",
    "    feature_combinations = featuresComblist(features)  # feature combinations\n",
    "    lookback_windows = [1, 3, 5, 7, 10, 15]\n",
    "    learning_rates = [0.1, 0.01, 0.001]\n",
    "    batch_sizes = [16, 32, 64]\n",
    "    epochs = [20, 50, 100]\n",
    "\n",
    "    # Load your data\n",
    "    filepath = './Data/EURUSD_D1.csv' \n",
    "    dataset = data_loader(filepath)\n",
    "    multiVarData = multivariateFeatureEngineering(dataset)\n",
    "    cols  = [col for col in multiVarData.columns if col!='Close'] + ['Close'] # Put target on End\n",
    "    multiVarData = multiVarData[cols]\n",
    "    #Shift clost to the end\n",
    "\n",
    "    print(multiVarData.head())\n",
    "    target_col = -1\n",
    "    \n",
    "    # Loop over all combinations of hyperparameters\n",
    "    for features_used in feature_combinations:\n",
    "        for lookback_window in lookback_windows:\n",
    "            for lr in learning_rates:\n",
    "                for batch_size in batch_sizes:\n",
    "                    for epoch in epochs:\n",
    "\n",
    "                        future_steps = [1, 3, 5]  # model outputs\n",
    "\n",
    "                        # Prepare features and response for the current hyperparameter combination\n",
    "                        features, response = multivariateFeatureLagMultiStep(multiVarData, lookback_window, future_steps, target_col)\n",
    "                        \n",
    "                        # Split data into training and testing sets\n",
    "                        X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.2, random_state=12, shuffle=False)\n",
    "\n",
    "                        # Standardize features\n",
    "                        scaler = StandardScaler()\n",
    "                        X_train = scaler.fit_transform(X_train)\n",
    "                        X_test = scaler.transform(X_test)\n",
    "\n",
    "                        # Train the model with the current hyperparameter combination\n",
    "                        model_lr = LinearRegressionMultiOutputBatch(lr, epoch, batch_size)\n",
    "                        model_lr.fit(X_train, Y_train)\n",
    "\n",
    "                        # Make predictions\n",
    "                        predictions = model_lr.predict(X_test)\n",
    "                        \n",
    "                        # Evaluate the model for each forecast horizon\n",
    "                        mse_1_day, mae_1_day, mape_1_day, mbe_1_day, rmse_1_day, r2_1_day = evaluate_model(Y_test[:, 0], predictions[:, 0])\n",
    "                        mse_3_day, mae_3_day, mape_3_day, mbe_3_day, rmse_3_day, r2_3_day = evaluate_model(Y_test[:, 1], predictions[:, 1])\n",
    "                        mse_5_day, mae_5_day, mape_5_day, mbe_5_day, rmse_5_day, r2_5_day = evaluate_model(Y_test[:, 2], predictions[:, 2])\n",
    "\n",
    "                        # Store the results\n",
    "                        result = {\n",
    "                            'lookback_window': lookback_window,\n",
    "                            'features_used': features_used,\n",
    "                            'learning_rate': lr,\n",
    "                            'batch_size': batch_size,\n",
    "                            'epochs': epoch,\n",
    "                            'MSE_1_day': mse_1_day,\n",
    "                            'MAE_1_day': mae_1_day,\n",
    "                            'MAPE_1_day': mape_1_day,\n",
    "                            'MBE_1_day': mbe_1_day,\n",
    "                            'RMSE_1_day': rmse_1_day,\n",
    "                            'R2_1_day': r2_1_day,\n",
    "                            'MSE_3_day': mse_3_day,\n",
    "                            'MAE_3_day': mae_3_day,\n",
    "                            'MAPE_3_day': mape_3_day,\n",
    "                            'MBE_3_day': mbe_3_day,\n",
    "                            'RMSE_3_day': rmse_3_day,\n",
    "                            'R2_3_day': r2_3_day,\n",
    "                            'MSE_5_day': mse_5_day,\n",
    "                            'MAE_5_day': mae_5_day,\n",
    "                            'MAPE_5_day': mape_5_day,\n",
    "                            'MBE_5_day': mbe_5_day,\n",
    "                            'RMSE_5_day': rmse_5_day,\n",
    "                            'R2_5_day': r2_5_day\n",
    "                        }\n",
    "\n",
    "                        # Append the result to the CSV file\n",
    "                        append_result_to_csv(file_name, result)\n",
    "\n",
    "    print(\"Grid search completed and results saved to CSV!\")\n",
    "\n",
    "\n",
    "############################### HYPER-PARAMETER TUNING ###############################\n",
    "\n",
    "grid_search() #start search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
