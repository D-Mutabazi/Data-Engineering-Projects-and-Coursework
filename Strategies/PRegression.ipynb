{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:501: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:1196: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:501: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:1196: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:501: RuntimeWarning: overflow encountered in square\n",
      "  output_errors = np.average((y_true - y_pred) ** 2, axis=0, weights=sample_weight)\n",
      "/home/dgibrilly/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:1196: RuntimeWarning: overflow encountered in square\n",
      "  numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0, dtype=np.float64)\n",
      "/tmp/ipykernel_1553276/1699587611.py:89: RuntimeWarning: invalid value encountered in subtract\n",
      "  self.W = self.W - self.learning_rate * (1 / X_batch.shape[0]) * np.dot(X_batch.T, error)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 365\u001b[0m\n\u001b[1;32m    359\u001b[0m                             append_result_to_csv(file_name, result)\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGrid search completed and results saved to CSV!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 365\u001b[0m \u001b[43mgrid_Search\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 327\u001b[0m, in \u001b[0;36mgrid_Search\u001b[0;34m()\u001b[0m\n\u001b[1;32m    324\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model_pr\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[1;32m    326\u001b[0m \u001b[38;5;66;03m# Evaluate the model for each forecast horizon\u001b[39;00m\n\u001b[0;32m--> 327\u001b[0m mse_1_day, mae_1_day, mape_1_day, mbe_1_day, rmse_1_day, r2_1_day \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    328\u001b[0m mse_3_day, mae_3_day, mape_3_day, mbe_3_day, rmse_3_day, r2_3_day \u001b[38;5;241m=\u001b[39m evaluate_model(Y_test[:, \u001b[38;5;241m1\u001b[39m], predictions[:, \u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    329\u001b[0m mse_5_day, mae_5_day, mape_5_day, mbe_5_day, rmse_5_day, r2_5_day \u001b[38;5;241m=\u001b[39m evaluate_model(Y_test[:, \u001b[38;5;241m2\u001b[39m], predictions[:, \u001b[38;5;241m2\u001b[39m])\n",
      "Cell \u001b[0;32mIn[15], line 199\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mevaluate_model\u001b[39m(y_true, y_pred):\n\u001b[0;32m--> 199\u001b[0m     mse \u001b[38;5;241m=\u001b[39m \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    200\u001b[0m     mae \u001b[38;5;241m=\u001b[39m mean_absolute_error(y_true, y_pred)\n\u001b[1;32m    201\u001b[0m     mape \u001b[38;5;241m=\u001b[39m mean_absolute_percentage_error(y_true, y_pred)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m    211\u001b[0m         )\n\u001b[1;32m    212\u001b[0m     ):\n\u001b[0;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[1;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[1;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[1;32m    223\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:497\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[0;34m(y_true, y_pred, sample_weight, multioutput, squared)\u001b[0m\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m squared:\n\u001b[1;32m    493\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m root_mean_squared_error(\n\u001b[1;32m    494\u001b[0m             y_true, y_pred, sample_weight\u001b[38;5;241m=\u001b[39msample_weight, multioutput\u001b[38;5;241m=\u001b[39mmultioutput\n\u001b[1;32m    495\u001b[0m         )\n\u001b[0;32m--> 497\u001b[0m y_type, y_true, y_pred, multioutput \u001b[38;5;241m=\u001b[39m \u001b[43m_check_reg_targets\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmultioutput\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    500\u001b[0m check_consistent_length(y_true, y_pred, sample_weight)\n\u001b[1;32m    501\u001b[0m output_errors \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39maverage((y_true \u001b[38;5;241m-\u001b[39m y_pred) \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, weights\u001b[38;5;241m=\u001b[39msample_weight)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/metrics/_regression.py:104\u001b[0m, in \u001b[0;36m_check_reg_targets\u001b[0;34m(y_true, y_pred, multioutput, dtype)\u001b[0m\n\u001b[1;32m    102\u001b[0m check_consistent_length(y_true, y_pred)\n\u001b[1;32m    103\u001b[0m y_true \u001b[38;5;241m=\u001b[39m check_array(y_true, ensure_2d\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m--> 104\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m y_true\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    107\u001b[0m     y_true \u001b[38;5;241m=\u001b[39m y_true\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:1049\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1043\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1044\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m expected <= 2.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1045\u001b[0m         \u001b[38;5;241m%\u001b[39m (array\u001b[38;5;241m.\u001b[39mndim, estimator_name)\n\u001b[1;32m   1046\u001b[0m     )\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m force_all_finite:\n\u001b[0;32m-> 1049\u001b[0m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1050\u001b[0m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1051\u001b[0m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mallow-nan\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1054\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[1;32m   1057\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[1;32m   1058\u001b[0m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:126\u001b[0m, in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m--> 126\u001b[0m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    127\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    131\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    132\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    133\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/utils/validation.py:175\u001b[0m, in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m     msg_err \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    162\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not accept missing values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m#estimators-that-handle-nan-values\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    174\u001b[0m     )\n\u001b[0;32m--> 175\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN."
     ]
    }
   ],
   "source": [
    "'''\n",
    "    this is the final polynomial regression models I am builing to get the required results\n",
    "'''\n",
    "\n",
    "############################################# Necessary dependancies #############################################\n",
    "import talib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "############################################ Class and helper functions ###########################################\n",
    "\n",
    "    \n",
    "class PolynomialRegressionMultiOutput:\n",
    "     \n",
    "    def __init__(self, degree, learning_rate, iterations, batch_size=None):\n",
    "        self.degree = degree\n",
    "        self.learning_rate = learning_rate\n",
    "        self.iterations = iterations\n",
    "        self.batch_size = batch_size\n",
    "        self.W = None\n",
    "        self.mean = None\n",
    "        self.std = None\n",
    "\n",
    "    # Function to transform X into polynomial features\n",
    "    def transform(self, X):\n",
    "        # Start with the bias term (X^0 = 1)\n",
    "        X_transform = np.ones((X.shape[0], 1))  # Shape: (n_samples, 1)\n",
    "        \n",
    "        # Loop through degrees from 1 to the specified degree\n",
    "        for j in range(1, self.degree + 1):\n",
    "            for feature in range(X.shape[1]):  # Loop through all features (columns) in X\n",
    "                X_pow = np.power(X[:, feature], j).reshape(-1, 1)  # Raise each feature to the power j\n",
    "                X_transform = np.hstack((X_transform, X_pow))  # Stack the new column horizontally\n",
    "            \n",
    "        return X_transform\n",
    "\n",
    "    # Function to normalize the features (save mean and std for normalization)\n",
    "    def normalize(self, X):\n",
    "        if self.mean is None or self.std is None:\n",
    "            self.mean = np.mean(X[:, 1:], axis=0)  # Mean excluding the bias term\n",
    "            self.std = np.std(X[:, 1:], axis=0)    # Standard deviation excluding the bias term\n",
    "        X[:, 1:] = (X[:, 1:] - self.mean) / self.std\n",
    "        return X\n",
    "\n",
    "    # Model training using mini-batch gradient descent\n",
    "    def fit(self, X, Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        self.m, self.n = self.X.shape\n",
    "        self.n_outputs = self.Y.shape[1]  # Number of output variables\n",
    "\n",
    "        # Transform X into polynomial features\n",
    "        X_transform = self.transform(self.X)\n",
    "\n",
    "        # Normalize the transformed features\n",
    "        X_normalize = self.normalize(X_transform)\n",
    "\n",
    "        # Initialize weights for multi-output (n_features, n_outputs)\n",
    "        self.W = np.zeros((X_normalize.shape[1], self.n_outputs))\n",
    "\n",
    "        # print(f'model training: x_normalize shape:{X_normalize.shape}\\nWeights shape: {self.W.shape}')\n",
    "        \n",
    "        # If no batch size is set, process the entire dataset at once (default to full-batch gradient descent)\n",
    "        if self.batch_size is None:\n",
    "            self.batch_size = self.m\n",
    "        \n",
    "        # Gradient descent learning with mini-batch training\n",
    "        for i in range(self.iterations):\n",
    "            # Iterate over mini-batches\n",
    "            for start in range(0, self.m, self.batch_size):\n",
    "                end = min(start + self.batch_size, self.m)\n",
    "                X_batch = X_normalize[start:end]\n",
    "                Y_batch = self.Y[start:end]\n",
    "\n",
    "                # Compute predictions for the mini-batch\n",
    "                h = np.dot(X_batch, self.W)\n",
    "\n",
    "                # Calculate error\n",
    "                error = h - Y_batch\n",
    "\n",
    "                # Update weights\n",
    "                self.W = self.W - self.learning_rate * (1 / X_batch.shape[0]) * np.dot(X_batch.T, error)\n",
    "\n",
    "                # if i == 0 and start == 0:\n",
    "                    # print(f'Initial batch error shape: {error.shape}, X_batch shape: {X_batch.shape}')\n",
    "\n",
    "        return self\n",
    "\n",
    "    # Predict function using the learned weights\n",
    "    def predict(self, X):\n",
    "        # Transform and normalize X for prediction\n",
    "        X_transform = self.transform(X)\n",
    "        X_normalize = self.normalize(X_transform)\n",
    "\n",
    "        # Ensure output is a matrix of shape (n_samples, n_outputs)\n",
    "        return np.dot(X_normalize, self.W)\n",
    "\n",
    "def multivariateFeatureEngineering(data):\n",
    "    \n",
    "    #Trend following Indicators:\n",
    "\n",
    "    #SMA - identofy long term trend\n",
    "    data['50_sma'] = data['Close'].rolling(window=50).mean() \n",
    "    data['200_sma'] = data['Close'].rolling(window=200).mean() \n",
    "\n",
    "    #EMA - trend analysis: more weight applied to recent points\n",
    "    data['50_ema'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "    data['100_ema'] = data['Close'].ewm(span=100, adjust=False).mean()\n",
    "\n",
    "    #MACD\n",
    "    data['12_ema'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['26_ema'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "\n",
    "    data['MACD_line'] = data['12_ema']-data['26_ema'] # calculate the MACD line\n",
    "    data['Signal_line'] = data['MACD_line'].ewm(span=9, adjust=False).mean() # 9-preiod ema signal calculated from the Macdline\n",
    "    # data['MACD_histogram'] = data['MACD_line'] - data['Signal_line']\n",
    "\n",
    "    #ADX\n",
    "    # Calculate ADX using TA-Lib (14-period by default)\n",
    "    data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Momentum indicators:\n",
    "\n",
    "    #RSI - 14-period\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    \n",
    "    #Stochastic Oscillator\n",
    "    data['stoch_k'], data['stoch_d'] = talib.STOCH(data['High'], data['Low'], data['Close'], fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    #Volatility indicators#:\n",
    "\n",
    "    #ATR -Default period for ATR is 14\n",
    "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    data = data.dropna() # drop rows that have NA\n",
    "\n",
    "    #drop certain featires\n",
    "    data = data.drop(columns=['12_ema', '26_ema'])\n",
    "\n",
    "    return data\n",
    "\n",
    "def multivariateFeatureLagMultiStep(data, n_past, future_steps, target_column=1):\n",
    "    features = []\n",
    "    response = []\n",
    "\n",
    "    max_future_step = max(future_steps)\n",
    "    num_features = data.shape[1]\n",
    "    group_feature_lags =  1 # change grouping of lagged features\n",
    "\n",
    "    # Adjust the loop to prevent index out of bounds\n",
    "    for i in range(n_past, len(data) - max_future_step + 1):\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "                \n",
    "            lagged_features = []\n",
    "\n",
    "            for feature_idx in range(num_features):\n",
    "                feature_lags = data.iloc[i - n_past:i, feature_idx].values \n",
    "                lagged_features.extend(feature_lags) \n",
    "\n",
    "        elif group_feature_lags==0:\n",
    "            features.append(data.iloc[i - n_past:i, :].values)  # Take all columns as features\n",
    "\n",
    "        # Use .iloc for integer-based indexing and .values to get a NumPy array\n",
    "\n",
    "        if group_feature_lags==1:\n",
    "            features.append(lagged_features)\n",
    "\n",
    "        # Extract the target values at specified future steps using .iloc\n",
    "        response.append([data.iloc[i + step - 1, target_column] for step in future_steps])\n",
    "\n",
    "    # Convert lists to NumPy arrays after the loop\n",
    "    features = np.array(features)  # Shape: (num_samples, n_past, num_features)\n",
    "    response = np.array(response)  # Shape: (num_samples, len(future_steps))\n",
    "\n",
    "    # Flatten the features to 2D array: (num_samples, n_past * num_features)\n",
    "    features_flat = features.reshape(features.shape[0], -1)\n",
    "\n",
    "    return features_flat, response\n",
    "\n",
    "\n",
    "# Data loading\n",
    "def data_loader(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index('Time', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to evaluate model predictions\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    mse = mean_squared_error(y_true, y_pred)\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    mape = mean_absolute_percentage_error(y_true, y_pred)\n",
    "    mbe = np.mean(y_pred - y_true)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2 = r2_score(y_true, y_pred)\n",
    "    \n",
    "    return mse, mae, mape, mbe, rmse, r2\n",
    "\n",
    "\n",
    "# Initialize CSV file and write headers\n",
    "def initialize_csv(file_name):\n",
    "    headers = ['lookback_window', 'features_used', 'learning_rate', 'degree', 'batch_size', 'epochs', \n",
    "               'MSE_1_day', 'MAE_1_day', 'MAPE_1_day', 'MBE_1_day', 'RMSE_1_day', 'R2_1_day',\n",
    "               'MSE_3_day', 'MAE_3_day', 'MAPE_3_day', 'MBE_3_day', 'RMSE_3_day', 'R2_3_day',\n",
    "               'MSE_5_day', 'MAE_5_day', 'MAPE_5_day', 'MBE_5_day', 'RMSE_5_day', 'R2_5_day']\n",
    "    df = pd.DataFrame(columns=headers)\n",
    "    df.to_csv(file_name, index=False)\n",
    "\n",
    "# Append a single result (row) to the CSV file\n",
    "def append_result_to_csv(file_name, result):\n",
    "    df = pd.DataFrame([result])  # Convert result dictionary to DataFrame\n",
    "    df.to_csv(file_name, mode='a', header=False, index=False)  # Append to CS\n",
    "\n",
    "#                                    /* Feature combinations*/\n",
    "def featuresComblist(features):\n",
    "    import itertools\n",
    "\n",
    "    initial_feature = ['Close'] # Starting with the closing price\n",
    "\n",
    "    # Get all combinations of the features list and add to the initial feature (Closing Price)\n",
    "    feature_combinations = []\n",
    "    for i in range(len(features) + 1):\n",
    "        for combination in itertools.combinations(features, i):\n",
    "            feature_combinations.append(list(combination)+ initial_feature )\n",
    "    \n",
    "    return feature_combinations\n",
    "\n",
    "######################################## Hyper-parameter tuninng #########################################\n",
    "\n",
    "\n",
    "filepath= './Data/EURUSD_D1.csv'\n",
    "dataset = data_loader(filepath)\n",
    "multiVarData = multivariateFeatureEngineering(dataset) #Include addtional features outside of OHLCV\n",
    "cols  = [col for col in multiVarData.columns if col!='Close'] + ['Close'] # Put target on End\n",
    "multiVarData = multiVarData[cols]\n",
    "\n",
    "\n",
    "# Parameters to optimise\n",
    "\n",
    "features=['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "       '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "       'stoch_d', 'ATR']\n",
    "feature_combinations = featuresComblist(features)  # feature combinations\n",
    "lookback_windows = [1, 3, 5, 7, 10, 15]\n",
    "learning_rates = [0.1, 0.01, 0.001]\n",
    "batch_sizes = [16, 32, 64, 128]\n",
    "epochs = [10, 20, 50, 100]\n",
    "polyn_degrees = [2, 3]\n",
    "\n",
    "target_col=-1\n",
    "\n",
    "future_steps = [1, 3, 5] \n",
    "\n",
    "\n",
    "\n",
    "########################################## Hyperparameter search ##########################################\n",
    "\n",
    "\n",
    "def grid_Search():\n",
    "    # Initialize CSV to store results\n",
    "    file_name = 'PR_hyperparameter_tuning_results.csv'\n",
    "    initialize_csv(file_name)\n",
    "\n",
    "    features=['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema',\n",
    "        '100_ema', 'MACD_line', 'Signal_line', 'ADX', 'RSI', 'stoch_k',\n",
    "        'stoch_d', 'ATR']\n",
    "    feature_combinations = featuresComblist(features)  # feature combinations\n",
    "    lookback_windows = [1, 3, 5, 7, 10, 15]\n",
    "    learning_rates = [0.1, 0.01, 0.001]\n",
    "    batch_sizes = [16, 32, 64, 128]\n",
    "    epochs = [10, 20, 50, 100]\n",
    "    polyn_degrees = [2, 3]\n",
    "\n",
    "\n",
    "    # Load your data\n",
    "    filepath = './Data/EURUSD_D1.csv' \n",
    "    dataset = data_loader(filepath)\n",
    "    multiVarData = multivariateFeatureEngineering(dataset)\n",
    "    cols  = [col for col in multiVarData.columns if col!='Close'] + ['Close'] # Put target on End\n",
    "    multiVarData = multiVarData[cols]\n",
    "\n",
    "    #Loop over all combinations of hyper-parameters\n",
    "    for features_used in feature_combinations:\n",
    "\n",
    "        multiVarData_subset = multiVarData[features_used] # select subset of data\n",
    "\n",
    "        for lookback_window in lookback_windows:\n",
    "\n",
    "            for lr in learning_rates:\n",
    "\n",
    "                for degree in polyn_degrees:\n",
    "\n",
    "                    for batch_size in batch_sizes:\n",
    "\n",
    "                        for epoch in epochs:\n",
    "\n",
    "                            future_steps = [1, 3, 5]  # model outputs\n",
    "\n",
    "                            # Prepare features and response for the current hyperparameter combination\n",
    "                            features, response = multivariateFeatureLagMultiStep(multiVarData_subset, lookback_window, future_steps, target_col)\n",
    "                        \n",
    "                             # Split data into training and testing sets\n",
    "                            X_train, X_test, Y_train, Y_test = train_test_split(features, response, test_size=0.2, random_state=12, shuffle=False)\n",
    "\n",
    "                            # Standardize features\n",
    "                            scaler = StandardScaler()\n",
    "                            X_train = scaler.fit_transform(X_train)\n",
    "                            X_test = scaler.transform(X_test)\n",
    "\n",
    "                            # Train the model with the current hyperparameter combination\n",
    "                            model_pr = PolynomialRegressionMultiOutput(degree, lr, epoch, batch_size)\n",
    "                            model_pr.fit(X_train, Y_train)\n",
    "\n",
    "                            # Make predictions\n",
    "                            predictions = model_pr.predict(X_test)\n",
    "                            \n",
    "                            #Error handling:\n",
    "                            predictions = np.nan_to_num(predictions, nan=50)\n",
    "                            #clip values to prevent overflow\n",
    "                            predictions = np.clip(predictions, -50, 50)\n",
    "\n",
    "                            # Evaluate the model for each forecast horizon\n",
    "                            mse_1_day, mae_1_day, mape_1_day, mbe_1_day, rmse_1_day, r2_1_day = evaluate_model(Y_test[:, 0], predictions[:, 0])\n",
    "                            mse_3_day, mae_3_day, mape_3_day, mbe_3_day, rmse_3_day, r2_3_day = evaluate_model(Y_test[:, 1], predictions[:, 1])\n",
    "                            mse_5_day, mae_5_day, mape_5_day, mbe_5_day, rmse_5_day, r2_5_day = evaluate_model(Y_test[:, 2], predictions[:, 2])\n",
    "                            #Store the results\n",
    "                            result = {\n",
    "                                'lookback_window': lookback_window,\n",
    "                                'features_used': features_used,\n",
    "                                'learning_rate': lr,\n",
    "                                'degree': degree,\n",
    "                                'batch_size': batch_size,\n",
    "                                'epochs': epoch,\n",
    "                                'MSE_1_day': mse_1_day,\n",
    "                                'MAE_1_day': mae_1_day,\n",
    "                                'MAPE_1_day': mape_1_day,\n",
    "                                'MBE_1_day': mbe_1_day,\n",
    "                                'RMSE_1_day': rmse_1_day,\n",
    "                                'R2_1_day': r2_1_day,\n",
    "                                'MSE_3_day': mse_3_day,\n",
    "                                'MAE_3_day': mae_3_day,\n",
    "                                'MAPE_3_day': mape_3_day,\n",
    "                                'MBE_3_day': mbe_3_day,\n",
    "                                'RMSE_3_day': rmse_3_day,\n",
    "                                'R2_3_day': r2_3_day,\n",
    "                                'MSE_5_day': mse_5_day,\n",
    "                                'MAE_5_day': mae_5_day,\n",
    "                                'MAPE_5_day': mape_5_day,\n",
    "                                'MBE_5_day': mbe_5_day,\n",
    "                                'RMSE_5_day': rmse_5_day,\n",
    "                                'R2_5_day': r2_5_day\n",
    "                            }\n",
    "\n",
    "                            # Append the result to the CSV file\n",
    "                            append_result_to_csv(file_name, result)\n",
    "\n",
    "    print(\"Grid search completed and results saved to CSV!\")\n",
    "\n",
    "\n",
    "\n",
    "grid_Search()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
