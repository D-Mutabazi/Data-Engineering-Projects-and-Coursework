{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/numpy/__init__.py:144\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m exceptions\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m dtypes\n\u001b[0;32m--> 144\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m lib\n\u001b[1;32m    145\u001b[0m \u001b[38;5;66;03m# NOTE: to be revisited following future namespace cleanup.\u001b[39;00m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;66;03m# See gh-14454 and gh-15672 for discussion.\u001b[39;00m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1027\u001b[0m, in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:1006\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:688\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:879\u001b[0m, in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:975\u001b[0m, in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap_external>:1074\u001b[0m, in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import r2_score\n",
    "import talib\n",
    "\n",
    "\n",
    "# Define the Linear Regression model\n",
    "class LinearRegression:\n",
    "    def __init__(self, learning_rate=0.01, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0\n",
    "\n",
    "        for _ in range(self.n_iterations):\n",
    "            # Calculate predictions\n",
    "            y_pred = np.dot(X, self.weights) + self.bias\n",
    "\n",
    "            # Compute gradients\n",
    "            dw = (1 / num_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1 / num_samples) * np.sum(y_pred - y)\n",
    "\n",
    "            # Update weights and bias\n",
    "            self.weights -= self.learning_rate * dw\n",
    "            self.bias -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "    \n",
    "\n",
    "    # Data loading\n",
    "def data_loader(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    data.set_index('Time', inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Feature Engineering\n",
    "\n",
    "def multivariateFeatureEngineering(filepath):\n",
    "    data = pd.read_csv(filepath)\n",
    "\n",
    "    data['Time'] = pd.to_datetime(data['Time'],format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    '''\n",
    "        Feature Egineering - Added features: MA, EMA, MACD, RSI\n",
    "    '''\n",
    "    #Trend following Indicators:\n",
    "\n",
    "    #SMA - identofy long term trend\n",
    "    data['50_sma'] = data['Close'].rolling(window=50).mean() \n",
    "    data['200_sma'] = data['Close'].rolling(window=200).mean() \n",
    "\n",
    "    #EMA - trend analysis: more weight applied to recent points\n",
    "    data['50_ema'] = data['Close'].ewm(span=50, adjust=False).mean()\n",
    "    data['100_ema'] = data['Close'].ewm(span=100, adjust=False).mean()\n",
    "\n",
    "    #MACD\n",
    "    data['12_ema'] = data['Close'].ewm(span=12, adjust=False).mean()\n",
    "    data['26_ema'] = data['Close'].ewm(span=26, adjust=False).mean()\n",
    "    data['MACD_line'] = data['12_ema']-data['26_ema'] # calculate the MACD line\n",
    "    data['Signal_line'] = data['MACD_line'].ewm(span=9, adjust=False).mean() # 9-preiod ema signal calculated from the Macdline\n",
    "    data['MACD_histogram'] = data['MACD_line'] - data['Signal_line']\n",
    "\n",
    "    #ADX\n",
    "    # Calculate ADX using TA-Lib (14-period by default)\n",
    "    data['ADX'] = talib.ADX(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Momentum indicators:\n",
    "\n",
    "    #RSI - 14-period\n",
    "    data['RSI'] = talib.RSI(data['Close'], timeperiod=14)\n",
    "    \n",
    "    #Stochastic Oscillator\n",
    "    data['stoch_k'], data['stoch_d'] = talib.STOCH(data['High'], data['Low'], data['Close'], \n",
    "                                                fastk_period=14, slowk_period=3, slowd_period=3)\n",
    "\n",
    "    #Williams %R - Default period is 14\n",
    "    data['Williams_R'] = talib.WILLR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "    #Volatility indicators#:\n",
    "\n",
    "    #Bollinger Bands - Default period is 20 for moving average and standard deviation (can be adjusted)\n",
    "    data['upper_band'], data['middle_band'], data['lower_band'] = talib.BBANDS(data['Close'], timeperiod=20, nbdevup=2, nbdevdn=2, matype=0)\n",
    "\n",
    "    #ATR -Default period for ATR is 14\n",
    "    data['ATR'] = talib.ATR(data['High'], data['Low'], data['Close'], timeperiod=14)\n",
    "\n",
    "   \n",
    "    data = data.dropna() # drop rows that have NA\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# reshape an input data into n_samples x timesteps x n_features.\n",
    "# NOTE: data should be standardized\n",
    "\n",
    "def multivariateFeatureLag(data, n_past, n_future, target_column=1):\n",
    "    features = []\n",
    "    response = []\n",
    "\n",
    "    for i in range(n_past, len(data) - n_future + 1):\n",
    "        # Extract the past n_past time steps as features\n",
    "        features.append(data[i - n_past:i, :])  # Take all columns as features\n",
    "\n",
    "        response.append(data[i + n_future - 1, target_column])  # Extract the target column for response\n",
    "\n",
    "    # Move the conversion to NumPy arrays outside the loop\n",
    "    features = np.array(features)  # Convert to NumPy array after the loop finishes\n",
    "    response = np.array(response)  # Convert to NumPy array after the loop finishes\n",
    "\n",
    "    features_flat = features.reshape(features.shape[0], -1) # make shape 2d\n",
    "\n",
    "    return features_flat, response\n",
    "\n",
    "\n",
    "\n",
    "import itertools\n",
    "import logging\n",
    "import talib\n",
    "import csv\n",
    "\n",
    "# Define the file and the header\n",
    "csv_file = 'hyperparameter_results.csv'\n",
    "header = ['Learning Rate', 'Lookback Window', 'Features', 'Test MSE', 'Test MAE','Test MBE', 'Test R2']\n",
    "\n",
    "\n",
    "features = ['Open', 'High', 'Low', 'Volume', '50_sma', '200_sma', '50_ema', \n",
    "                '100_ema', '12_ema', '26_ema', 'MACD_line', 'Signal_line', \n",
    "                'MACD_histogram', 'ADX', 'RSI', 'stoch_k', 'stoch_d', 'Williams_R',\n",
    "                'ATR']\n",
    "\n",
    "\n",
    "learning_rates = [0.1 ,0.01, 0.001]  \n",
    "lookback_windows = [5, 7, 10, 15,20]\n",
    "n_future = 1\n",
    "target_col = -1         \n",
    "\n",
    "initial_feature = ['Close'] # Starting with the closing price\n",
    "\n",
    "# Get all combinations of the features list and add to the initial feature (Closing Price)\n",
    "feature_combinations = []\n",
    "for i in range(len(features) + 1):\n",
    "    for combination in itertools.combinations(features, i):\n",
    "        feature_combinations.append(list(combination)+ initial_feature )\n",
    "\n",
    "print(f\"Total number of combinations: {len(feature_combinations)}\")\n",
    "\n",
    "filepath = './Data/EURUSD_D1.csv'\n",
    "\n",
    "multivariateDataX = multivariateFeatureEngineering(filepath) #Include addtional features outside of OHLCV\n",
    "cols = [col for col in multivariateDataX.columns if col!='Close'] +['Close'] #shift Close to end\n",
    "multivariateDataX =multivariateDataX[cols]\n",
    "multivariateDataX = multivariateDataX.drop(columns=['Time']).reset_index(drop=True) #drop time column reset index\n",
    "\n",
    "\n",
    "# Grid search loop\n",
    "best_model = None\n",
    "best_mae = 1 # Where the previous program crashed\n",
    "with open(csv_file, mode='w', newline='') as file:\n",
    "  writer = csv.writer(file)\n",
    "    \n",
    "  writer.writerow(header) # header\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for n_past_window_size in lookback_windows:\n",
    "        for features in feature_combinations:\n",
    "            # update dataset to contain only features selected\n",
    "            multivariateDataNew = multivariateDataX[list(features)]\n",
    "\n",
    "            m_data = multivariateDataNew.to_numpy()\n",
    "\n",
    "            # Create lagged features based on the window size\n",
    "            m_X, m_Y = multivariateFeatureLag(m_data,n_past_window_size, n_future, target_col )\n",
    "\n",
    "            # Train/test split\n",
    "            X_train, X_test, y_train, y_test = train_test_split(m_X, m_Y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "            # Scale features\n",
    "            scaler = StandardScaler()\n",
    "            X_train = scaler.fit_transform(X_train)\n",
    "            X_test = scaler.transform(X_test)\n",
    "\n",
    "            # Fit model\n",
    "            model = LinearRegression(lr, n_iterations=1000)\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions\n",
    "            y_pred_train = model.predict(X_train)\n",
    "            y_pred_test = model.predict(X_test)\n",
    "\n",
    "            # Check for NaN in predictions and handle them (e.g., replace with 0)\n",
    "            y_pred_test = np.nan_to_num(y_pred_test)\n",
    "\n",
    "            # Calculate metrics\n",
    "            test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "            test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "            test_mbe = np.mean(y_pred_test - y_test) # mean bias error\n",
    "            test_r2 = r2_score(y_test, y_pred_test)\n",
    "            # MAPE,\n",
    "\n",
    "            # store improving models\n",
    "            if test_mae < best_mae:\n",
    "                best_mae = test_mae\n",
    "                 # Log results to CSV\n",
    "                with open(csv_file, mode='a', newline='') as file:\n",
    "                  writer = csv.writer(file)\n",
    "                  writer.writerow([lr, n_past_window_size, features, test_mse, test_mae,test_mbe, test_r2])  # Write the result row\n",
    "\n",
    "\n",
    "# Save the best model (you can save the model using joblib or pickle)\n",
    "print(f\"Best model found with Test MSE: {best_mae}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
